"use strict";(self.webpackChunkplantacerium=self.webpackChunkplantacerium||[]).push([["583"],{6117:function(e,s,n){n.r(s),n.d(s,{default:()=>a});var l=n(5893),r=n(65);function i(e){let s={a:"a",code:"code",em:"em",h1:"h1",h3:"h3",h4:"h4",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",span:"span",strong:"strong",ul:"ul",...(0,r.a)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsxs)(s.h1,{id:"ai-ram-ollama-continue-dev",children:[(0,l.jsx)(s.a,{className:"header-anchor","aria-hidden":"true",href:"#ai-ram-ollama-continue-dev",children:"#"}),"AI Ram Ollama Continue Dev"]}),"\n",(0,l.jsxs)(s.p,{children:["A ",(0,l.jsx)(s.code,{children:"llamafile"})," is just a single, executable file that bundles the ",(0,l.jsx)(s.code,{children:"llama.cpp"})," engine with model weights."]}),"\n",(0,l.jsxs)(s.p,{children:["However, for a 30GB+ model like our ",(0,l.jsx)(s.code,{children:"Q8_0"})," GGUF, creating a 30GB executable is impractical. The ",(0,l.jsx)(s.em,{children:"real"})," power-user workflow, which perfectly suits your goal, is to use the ",(0,l.jsx)(s.code,{children:"llamafile"})," executable ",(0,l.jsx)(s.em,{children:"as a portable server"})," and tell it to load an ",(0,l.jsx)(s.em,{children:"external"})," GGUF file."]}),"\n",(0,l.jsx)(s.p,{children:"This gives you the best of all worlds:"}),"\n",(0,l.jsxs)(s.ul,{children:["\n",(0,l.jsxs)(s.li,{children:[(0,l.jsx)(s.strong,{children:"Portability:"})," A single, ~15MB ",(0,l.jsx)(s.code,{children:"llamafile"})," executable that you can drop on any (Linux/macOS/Windows) machine."]}),"\n",(0,l.jsxs)(s.li,{children:[(0,l.jsx)(s.strong,{children:"Power:"})," You can load any GGUF file you want, including our 30GB ",(0,l.jsx)(s.code,{children:"gemma-3-27b-it-q8_0"}),"."]}),"\n",(0,l.jsxs)(s.li,{children:[(0,l.jsx)(s.strong,{children:"Performance:"})," You can pass all the ",(0,l.jsx)(s.code,{children:"llama.cpp"})," performance flags (",(0,l.jsx)(s.code,{children:"--mlock"}),", ",(0,l.jsx)(s.code,{children:"--threads"}),", ",(0,l.jsx)(s.code,{children:"--n-gpu-layers"}),") directly to it."]}),"\n",(0,l.jsxs)(s.li,{children:[(0,l.jsx)(s.strong,{children:"Customization:"})," You can apply LoRA adapters on the fly."]}),"\n",(0,l.jsxs)(s.li,{children:[(0,l.jsx)(s.strong,{children:"Integration:"})," It starts an OpenAI-compatible server, which is ",(0,l.jsx)(s.em,{children:"exactly"})," what tools like ",(0,l.jsx)(s.code,{children:"Continue.dev"})," need."]}),"\n"]}),"\n",(0,l.jsx)(s.p,{children:"Here is the complete walkthrough to create the ultimate, portable, high-performance coding experience."}),"\n",(0,l.jsx)(s.hr,{}),"\n",(0,l.jsxs)(s.h3,{id:"-the-portable-powerhouse-llamafile-walkthrough",children:[(0,l.jsx)(s.a,{className:"header-anchor","aria-hidden":"true",href:"#-the-portable-powerhouse-llamafile-walkthrough",children:"#"}),'## The "Portable Powerhouse" ',(0,l.jsx)(s.code,{children:"llamafile"})," Walkthrough"]}),"\n",(0,l.jsxs)(s.h4,{id:"-phase-1-acquire-your-engine-and-fuel",children:[(0,l.jsx)(s.a,{className:"header-anchor","aria-hidden":"true",href:"#-phase-1-acquire-your-engine-and-fuel",children:"#"}),'### Phase 1: Acquire Your "Engine" and "Fuel"']}),"\n",(0,l.jsxs)(s.p,{children:["We need two things: the ",(0,l.jsx)(s.code,{children:"llamafile"})," executable (the engine) and our high-fidelity model (the fuel)."]}),"\n",(0,l.jsxs)(s.ol,{children:["\n",(0,l.jsxs)(s.li,{children:["\n",(0,l.jsxs)(s.p,{children:[(0,l.jsxs)(s.strong,{children:["Download the ",(0,l.jsx)(s.code,{children:"llamafile"})," Executable:"]}),"\nGo to the ",(0,l.jsx)(s.a,{href:"https://github.com/Mozilla-Ocho/llamafile/releases",rel:"noopener noreferrer",target:"_blank",children:"llamafile GitHub releases page"})," and download the main ",(0,l.jsx)(s.code,{children:"llamafile-0.8.6"})," (or newer) executable. We don't need a model bundled with it."]}),"\n",(0,l.jsx)(l.Fragment,{children:(0,l.jsx)(s.pre,{className:"shiki css-variables",style:{backgroundColor:"var(--shiki-background)",color:"var(--shiki-foreground)"},tabIndex:"0",children:(0,l.jsxs)(s.code,{className:"language-bash",children:[(0,l.jsx)(s.span,{className:"line",children:(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-comment)"},children:"# Create a workspace"})}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-function)"},children:"mkdir"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string)"},children:" -p"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string)"},children:" ~/llamafile-power-setup"})]}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-function)"},children:"cd"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string)"},children:" ~/llamafile-power-setup"})]}),"\n",(0,l.jsx)(s.span,{className:"line"}),"\n",(0,l.jsx)(s.span,{className:"line",children:(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-comment)"},children:"# Download the llamafile binary"})}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-function)"},children:"wget"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string)"},children:" https://github.com/Mozilla-Ocho/llamafile/releases/download/0.8.6/llamafile-0.8.6"})]}),"\n",(0,l.jsx)(s.span,{className:"line"}),"\n",(0,l.jsx)(s.span,{className:"line",children:(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-comment)"},children:"# Make it executable (Linux/macOS)"})}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-function)"},children:"chmod"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string)"},children:" +x"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string)"},children:" llamafile-0.8.6"})]}),"\n",(0,l.jsx)(s.span,{className:"line"}),"\n",(0,l.jsx)(s.span,{className:"line",children:(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-comment)"},children:'# On Windows, you would just rename it to "llamafile-0.8.6.exe"'})})]})})}),"\n",(0,l.jsx)(s.p,{children:"You now have your portable engine."}),"\n"]}),"\n",(0,l.jsxs)(s.li,{children:["\n",(0,l.jsxs)(s.p,{children:[(0,l.jsx)(s.strong,{children:"Download the High-Fidelity GGUF Model:"}),"\nThis is the same as our previous step. We'll download the 30GB ",(0,l.jsx)(s.code,{children:"Q8_0"})," model and place it in a ",(0,l.jsx)(s.code,{children:"models"})," folder."]}),"\n",(0,l.jsx)(l.Fragment,{children:(0,l.jsx)(s.pre,{className:"shiki css-variables",style:{backgroundColor:"var(--shiki-background)",color:"var(--shiki-foreground)"},tabIndex:"0",children:(0,l.jsxs)(s.code,{className:"language-bash",children:[(0,l.jsx)(s.span,{className:"line",children:(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-comment)"},children:"# Install the tool to download from Hugging Face"})}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-function)"},children:"pip"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string)"},children:" install"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string)"},children:" -U"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string)"},children:" huggingface-cli"})]}),"\n",(0,l.jsx)(s.span,{className:"line"}),"\n",(0,l.jsx)(s.span,{className:"line",children:(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-comment)"},children:"# Create a models directory"})}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-function)"},children:"mkdir"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string)"},children:" -p"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string)"},children:" ./models"})]}),"\n",(0,l.jsx)(s.span,{className:"line"}),"\n",(0,l.jsx)(s.span,{className:"line",children:(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-comment)"},children:"# Download our 30GB Q8_0 model"})}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-function)"},children:"huggingface-cli"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string)"},children:" download"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-foreground)"},children:" \\"})]}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string)"},children:"  paultimothymooney/gemma-3-27b-it-Q8_0-GGUF"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-foreground)"},children:" \\"})]}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string)"},children:"  gemma-3-27b-it-q8_0.gguf"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-foreground)"},children:" \\"})]}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string)"},children:"  --local-dir"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string)"},children:" ./models"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-foreground)"},children:" \\"})]}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string)"},children:"  --local-dir-use-symlinks"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string)"},children:" False"})]})]})})}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(s.h4,{id:"-phase-2-launch-the-server-with-max-performance-flags",children:[(0,l.jsx)(s.a,{className:"header-anchor","aria-hidden":"true",href:"#-phase-2-launch-the-server-with-max-performance-flags",children:"#"}),"### Phase 2: Launch the Server with Max Performance Flags"]}),"\n",(0,l.jsxs)(s.p,{children:["This is the core of the setup. We will run our ",(0,l.jsx)(s.code,{children:"llamafile"})," executable and pass it all the high-performance ",(0,l.jsx)(s.code,{children:"llama.cpp"})," flags."]}),"\n",(0,l.jsxs)(s.p,{children:["First, ",(0,l.jsx)(s.strong,{children:"find your PHYSICAL core count"})," (e.g., ",(0,l.jsx)(s.code,{children:"sysctl -n hw.physicalcpu"})," on macOS or ",(0,l.jsx)(s.code,{children:'lscpu | grep "Core(s) per socket"'})," on Linux). We'll use ",(0,l.jsx)(s.strong,{children:(0,l.jsx)(s.code,{children:"8"})})," cores as our example."]}),"\n",(0,l.jsx)(s.p,{children:"Here is the full launch command:"}),"\n",(0,l.jsx)(l.Fragment,{children:(0,l.jsx)(s.pre,{className:"shiki css-variables",style:{backgroundColor:"var(--shiki-background)",color:"var(--shiki-foreground)"},tabIndex:"0",children:(0,l.jsxs)(s.code,{className:"language-bash",children:[(0,l.jsx)(s.span,{className:"line",children:(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-comment)"},children:"# Run this command from your '~/llamafile-power-setup' directory"})}),"\n",(0,l.jsx)(s.span,{className:"line",children:(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-comment)"},children:"# This launches the OpenAI-compatible server"})}),"\n",(0,l.jsx)(s.span,{className:"line"}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-function)"},children:"./llamafile-0.8.6"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-foreground)"},children:" \\"})]}),"\n",(0,l.jsx)(s.span,{className:"line",children:(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-comment)"},children:"    # --- Server Flags ---"})}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-function)"},children:"    --server"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-foreground)"},children:" \\"})]}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string)"},children:"    --port"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-constant)"},children:" 8080"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-foreground)"},children:" \\"})]}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string)"},children:"    --host"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-constant)"},children:" 0.0.0.0"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-foreground)"},children:" \\"})]}),"\n",(0,l.jsx)(s.span,{className:"line",children:(0,l.jsx)(s.span,{style:{color:"var(--shiki-foreground)"},children:"    "})}),"\n",(0,l.jsx)(s.span,{className:"line",children:(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-comment)"},children:"    # --- Model Flags ---"})}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-function)"},children:"    -m"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string)"},children:" ./models/gemma-3-27b-it-q8_0.gguf"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-foreground)"},children:" \\"})]}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string)"},children:"    -c"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-constant)"},children:" 131072"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-foreground)"},children:" \\"})]}),"\n",(0,l.jsx)(s.span,{className:"line",children:(0,l.jsx)(s.span,{style:{color:"var(--shiki-foreground)"},children:"    "})}),"\n",(0,l.jsx)(s.span,{className:"line",children:(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-comment)"},children:"    # --- Performance Flags (CPU/RAM/GPU) ---"})}),"\n",(0,l.jsx)(s.span,{className:"line",children:(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-comment)"},children:"    # --mlock: Force model into RAM. CRITICAL for performance."})}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-function)"},children:"    --mlock"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-foreground)"},children:" \\"})]}),"\n",(0,l.jsx)(s.span,{className:"line",children:(0,l.jsx)(s.span,{style:{color:"var(--shiki-foreground)"},children:"    "})}),"\n",(0,l.jsx)(s.span,{className:"line",children:(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-comment)"},children:"    # -t: Thread count. Set to your PHYSICAL core count."})}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-function)"},children:"    -t"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-constant)"},children:" 8"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-foreground)"},children:" \\"})]}),"\n",(0,l.jsx)(s.span,{className:"line",children:(0,l.jsx)(s.span,{style:{color:"var(--shiki-foreground)"},children:"    "})}),"\n",(0,l.jsx)(s.span,{className:"line",children:(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-comment)"},children:"    # -ngl 99: Offload 99 layers to the GPU."})}),"\n",(0,l.jsx)(s.span,{className:"line",children:(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-comment)"},children:"    # If you have a GPU, this is essential."})}),"\n",(0,l.jsx)(s.span,{className:"line",children:(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-comment)"},children:"    # If you are CPU-only, set this to 0 (or just omit it)."})}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-function)"},children:"    --n-gpu-layers"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-constant)"},children:" 99"})]})]})})}),"\n",(0,l.jsxs)(s.p,{children:["Your terminal will now show server logs. You have a high-performance, OpenAI-compatible API running at ",(0,l.jsx)(s.code,{children:"http://127.0.0.1:8080"}),"."]}),"\n",(0,l.jsxs)(s.h4,{id:"-phase-3-customize-for-code-apply-lora",children:[(0,l.jsx)(s.a,{className:"header-anchor","aria-hidden":"true",href:"#-phase-3-customize-for-code-apply-lora",children:"#"}),"### Phase 3: Customize for Code (Apply LoRA)"]}),"\n",(0,l.jsxs)(s.p,{children:["This is what makes the ",(0,l.jsx)(s.code,{children:"llamafile"}),' server so powerful. You don\'t need to build a new model. You can "hot-swap" a LoRA by just adding a flag at launch.']}),"\n",(0,l.jsxs)(s.p,{children:["Let's assume you've downloaded a ",(0,l.jsx)(s.code,{children:"rust-code-lora.gguf"})," into your ",(0,l.jsx)(s.code,{children:"./models"})," folder."]}),"\n",(0,l.jsxs)(s.p,{children:["You would simply ",(0,l.jsx)(s.strong,{children:"add one flag"})," to the launch command:"]}),"\n",(0,l.jsx)(l.Fragment,{children:(0,l.jsx)(s.pre,{className:"shiki css-variables",style:{backgroundColor:"var(--shiki-background)",color:"var(--shiki-foreground)"},tabIndex:"0",children:(0,l.jsxs)(s.code,{className:"language-bash",children:[(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-function)"},children:"./llamafile-0.8.6"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-foreground)"},children:" \\"})]}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string)"},children:"    --server"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-foreground)"},children:" \\"})]}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string)"},children:"    --port"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-constant)"},children:" 8080"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-foreground)"},children:" \\"})]}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string)"},children:"    -m"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string)"},children:" ./models/gemma-3-27b-it-q8_0.gguf"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-foreground)"},children:" \\"})]}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string)"},children:"    -c"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-constant)"},children:" 131072"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-foreground)"},children:" \\"})]}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string)"},children:"    --mlock"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-foreground)"},children:" \\"})]}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string)"},children:"    -t"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-constant)"},children:" 8"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-foreground)"},children:" \\"})]}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string)"},children:"    --n-gpu-layers"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-constant)"},children:" 99"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-foreground)"},children:" \\"})]}),"\n",(0,l.jsx)(s.span,{className:"line",children:(0,l.jsx)(s.span,{style:{color:"var(--shiki-foreground)"},children:"    "})}),"\n",(0,l.jsx)(s.span,{className:"line",children:(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-comment)"},children:"    # --- The LoRA Customization ---"})}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-function)"},children:"    --lora"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string)"},children:" ./models/rust-code-lora.gguf"})]})]})})}),"\n",(0,l.jsxs)(s.p,{children:["Now, the server running at ",(0,l.jsx)(s.code,{children:"http://127.0.0.1:8080"})," is serving your ",(0,l.jsx)(s.code,{children:"gemma-q8"})," model ",(0,l.jsx)(s.em,{children:"already specialized for Rust"}),'. You can have multiple scripts to launch different "specialist" servers.']}),"\n",(0,l.jsxs)(s.h4,{id:"-phase-4-integrate-with-continuedev-for-the-best-code-experience",children:[(0,l.jsx)(s.a,{className:"header-anchor","aria-hidden":"true",href:"#-phase-4-integrate-with-continuedev-for-the-best-code-experience",children:"#"}),"### Phase 4: Integrate with ",(0,l.jsx)(s.code,{children:"Continue.dev"})," for the Best Code Experience"]}),"\n",(0,l.jsxs)(s.p,{children:["This is the final step. We will point ",(0,l.jsx)(s.code,{children:"Continue.dev"})," at our new, high-performance ",(0,l.jsx)(s.code,{children:"llamafile"})," server."]}),"\n",(0,l.jsxs)(s.p,{children:[(0,l.jsx)(s.code,{children:"Continue.dev"})," can connect to any OpenAI-compatible API."]}),"\n",(0,l.jsxs)(s.ol,{children:["\n",(0,l.jsxs)(s.li,{children:["\n",(0,l.jsxs)(s.p,{children:["Open VS Code and go to your ",(0,l.jsx)(s.code,{children:"~/.continue/config.yaml"})," file."]}),"\n"]}),"\n",(0,l.jsxs)(s.li,{children:["\n",(0,l.jsx)(s.p,{children:"Paste in this configuration:"}),"\n",(0,l.jsx)(l.Fragment,{children:(0,l.jsx)(s.pre,{className:"shiki css-variables",style:{backgroundColor:"var(--shiki-background)",color:"var(--shiki-foreground)"},tabIndex:"0",children:(0,l.jsxs)(s.code,{className:"language-yaml",children:[(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-keyword)"},children:"models"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-keyword)"},children:":"})]}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-foreground)"},children:"  - "}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-keyword)"},children:"name"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-keyword)"},children:":"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string-expression)"},children:' "llamafile-coder"'})]}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-keyword)"},children:"    title"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-keyword)"},children:":"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string-expression)"},children:' "Gemma 3 27B Coder (llamafile)"'})]}),"\n",(0,l.jsx)(s.span,{className:"line",children:(0,l.jsx)(s.span,{style:{color:"var(--shiki-foreground)"},children:"    "})}),"\n",(0,l.jsx)(s.span,{className:"line",children:(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-comment)"},children:'    # We use the "openai" provider, as llamafile emulates it'})}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-keyword)"},children:"    provider"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-keyword)"},children:":"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string-expression)"},children:' "openai"'})]}),"\n",(0,l.jsx)(s.span,{className:"line",children:(0,l.jsx)(s.span,{style:{color:"var(--shiki-foreground)"},children:"    "})}),"\n",(0,l.jsx)(s.span,{className:"line",children:(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-comment)"},children:"    # This is just a label for the API, it can be anything"})}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-keyword)"},children:"    model"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-keyword)"},children:":"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string-expression)"},children:' "gemma-3-27b-it-q8_0"'}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-foreground)"},children:" "})]}),"\n",(0,l.jsx)(s.span,{className:"line",children:(0,l.jsx)(s.span,{style:{color:"var(--shiki-foreground)"},children:"    "})}),"\n",(0,l.jsx)(s.span,{className:"line",children:(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-comment)"},children:"    # --- The Integration ---"})}),"\n",(0,l.jsx)(s.span,{className:"line",children:(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-comment)"},children:"    # Point to your local llamafile server"})}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-keyword)"},children:"    apiBase"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-keyword)"},children:":"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string-expression)"},children:' "http://127.0.0.1:8080/v1"'}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-foreground)"},children:" "})]}),"\n",(0,l.jsx)(s.span,{className:"line",children:(0,l.jsx)(s.span,{style:{color:"var(--shiki-foreground)"},children:"    "})}),"\n",(0,l.jsx)(s.span,{className:"line",children:(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-comment)"},children:"    # The API key can be any non-empty string"})}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-keyword)"},children:"    apiKey"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-keyword)"},children:":"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string-expression)"},children:' "llamafile"'}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-foreground)"},children:" "})]}),"\n",(0,l.jsx)(s.span,{className:"line"}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-foreground)"},children:"  - "}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-keyword)"},children:"name"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-keyword)"},children:":"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string-expression)"},children:' "Local Embedder (SOTA)"'})]}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-keyword)"},children:"    title"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-keyword)"},children:":"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string-expression)"},children:' "MixedBread Embed Large"'})]}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-keyword)"},children:"    provider"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-keyword)"},children:":"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string-expression)"},children:" ollama"})]}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-keyword)"},children:"    model"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-keyword)"},children:":"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string-expression)"},children:' "mxbai-embed-large-v1:latest"'}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-foreground)"},children:" "})]}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-keyword)"},children:"    apiBase"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-keyword)"},children:":"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string-expression)"},children:' "http://localhost:11434"'})]}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-keyword)"},children:"    embed"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-keyword)"},children:":"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-constant)"},children:" true"})]}),"\n",(0,l.jsx)(s.span,{className:"line"}),"\n",(0,l.jsx)(s.span,{className:"line",children:(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-comment)"},children:"# --- Set your models as default ---"})}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-keyword)"},children:"modelRoles"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-keyword)"},children:":"})]}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-keyword)"},children:"  chat"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-keyword)"},children:":"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string-expression)"},children:' "llamafile-coder"'})]}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-keyword)"},children:"  edit"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-keyword)"},children:":"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string-expression)"},children:' "llamafile-coder"'})]}),"\n",(0,l.jsx)(s.span,{className:"line",children:(0,l.jsx)(s.span,{style:{color:"var(--shiki-foreground)"},children:"  "})}),"\n",(0,l.jsxs)(s.span,{className:"line",children:[(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-keyword)"},children:"embeddingsProvider"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-keyword)"},children:":"}),(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-string-expression)"},children:' "Local Embedder (SOTA)"'})]}),"\n",(0,l.jsx)(s.span,{className:"line"}),"\n",(0,l.jsx)(s.span,{className:"line",children:(0,l.jsx)(s.span,{style:{color:"var(--shiki-token-comment)"},children:"# ... (rest of your contextProviders config) ..."})})]})})}),"\n",(0,l.jsx)(s.p,{children:(0,l.jsxs)(s.em,{children:["(Note: This setup still uses your Ollama server for embeddings, as it's the simplest way to manage the ",(0,l.jsx)(s.code,{children:"mxbai-embed-large"})," model. Your ",(0,l.jsx)(s.code,{children:"llamafile"})," server will handle all the ",(0,l.jsx)(s.em,{children:"generation"}),".)"]})}),"\n"]}),"\n",(0,l.jsxs)(s.li,{children:["\n",(0,l.jsx)(s.p,{children:(0,l.jsx)(s.strong,{children:"Reload VS Code."})}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(s.p,{children:["You are now 100% operational. When you type ",(0,l.jsx)(s.code,{children:"@codebase"})," in ",(0,l.jsx)(s.code,{children:"Continue.dev"}),":"]}),"\n",(0,l.jsxs)(s.ol,{children:["\n",(0,l.jsxs)(s.li,{children:[(0,l.jsx)(s.code,{children:"Continue.dev"})," uses your ",(0,l.jsx)(s.strong,{children:"Ollama"})," ",(0,l.jsx)(s.code,{children:"mxbai-embed-large"})," model to index your code."]}),"\n",(0,l.jsx)(s.li,{children:"It finds the relevant code snippets."}),"\n",(0,l.jsxs)(s.li,{children:["It sends the prompt and code context to your ",(0,l.jsxs)(s.strong,{children:[(0,l.jsx)(s.code,{children:"llamafile"})," server"]})," at ",(0,l.jsx)(s.code,{children:"http://127.0.0.1:8080"}),"."]}),"\n",(0,l.jsxs)(s.li,{children:["Your ",(0,l.jsx)(s.code,{children:"llamafile"})," server, running with locked RAM and full GPU/CPU acceleration, generates the code response using the high-fidelity ",(0,l.jsx)(s.code,{children:"gemma-q8"})," model (with the Rust LoRA, if you added it)."]}),"\n"]}),"\n",(0,l.jsxs)(s.p,{children:["You have successfully combined the raw power of a natively-run ",(0,l.jsx)(s.code,{children:"llama.cpp"})," engine with the simplicity of a ",(0,l.jsx)(s.code,{children:"llamafile"})," server and the deep integration of ",(0,l.jsx)(s.code,{children:"Continue.dev"}),"."]})]})}function a(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:s}={...(0,r.a)(),...e.components};return s?(0,l.jsx)(s,{...e,children:(0,l.jsx)(i,{...e})}):i(e)}a.__RSPRESS_PAGE_META={},a.__RSPRESS_PAGE_META["blog-software-engineer%2FTools%20Resources%2Fai-ram-ollama-continuedev-customized.md"]={toc:[{id:"-the-portable-powerhouse-llamafile-walkthrough",text:'## The "Portable Powerhouse" `llamafile` Walkthrough',depth:3},{id:"-phase-1-acquire-your-engine-and-fuel",text:'### Phase 1: Acquire Your "Engine" and "Fuel"',depth:4},{id:"-phase-2-launch-the-server-with-max-performance-flags",text:"### Phase 2: Launch the Server with Max Performance Flags",depth:4},{id:"-phase-3-customize-for-code-apply-lora",text:"### Phase 3: Customize for Code (Apply LoRA)",depth:4},{id:"-phase-4-integrate-with-continuedev-for-the-best-code-experience",text:"### Phase 4: Integrate with `Continue.dev` for the Best Code Experience",depth:4}],title:"AI Ram Ollama Continue Dev",headingTitle:"AI Ram Ollama Continue Dev",frontmatter:{}}}}]);